{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b97258",
   "metadata": {},
   "source": [
    "# 30天Python编程挑战：第22天 - 网页抓取\n",
    "\n",
    "- [第22天](#-第22天)\n",
    "  - [Python网页抓取](#python网页抓取)\n",
    "    - [什么是网页抓取](#什么是网页抓取)\n",
    "  - [💻 练习：第22天](#-练习第22天)\n",
    "\n",
    "# 📘 第22天\n",
    "\n",
    "## Python网页抓取\n",
    "\n",
    "### 什么是网页抓取\n",
    "\n",
    "互联网充满了大量的数据，这些数据可以用于不同的目的。要收集这些数据，我们需要知道如何从网站上抓取数据。\n",
    "\n",
    "网页抓取是从网站提取和收集数据，并将其存储在本地机器或数据库中的过程。\n",
    "\n",
    "在本节中，我们将使用beautifulsoup和requests包来抓取数据。我们使用的是beautifulsoup 4版本。\n",
    "\n",
    "要开始抓取网站，你需要_requests_、_beautifoulSoup4_和一个_网站_。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12500c2",
   "metadata": {
    "attributes": {
     "classes": [
      "sh"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install requests\n",
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c678f29",
   "metadata": {},
   "source": [
    "要从网站抓取数据，需要基本了解HTML标签和CSS选择器。我们使用HTML标签、类或/和ID来定位网站上的内容。\n",
    "让我们导入requests和BeautifulSoup模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc3bd2",
   "metadata": {
    "attributes": {
     "classes": [
      "py"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6011b5e",
   "metadata": {},
   "source": [
    "让我们声明一个url变量，用于我们要抓取的网站。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cf5f6",
   "metadata": {
    "attributes": {
     "classes": [
      "py"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "# 让我们使用requests的get方法从url获取数据\n",
    "response = requests.get(url)\n",
    "# 检查状态\n",
    "status = response.status_code\n",
    "print(status) # 200表示获取成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298fe672",
   "metadata": {
    "attributes": {
     "classes": [
      "sh"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1768096",
   "metadata": {},
   "source": [
    "使用beautifulSoup解析页面内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a758c",
   "metadata": {
    "attributes": {
     "classes": [
      "py"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://archive.ics.uci.edu/ml/datasets.php'\n",
    "\n",
    "response = requests.get(url)\n",
    "content = response.content # 我们从网站获取所有内容\n",
    "soup = BeautifulSoup(content, 'html.parser') # beautiful soup将给我们一个解析的机会\n",
    "print(soup.title) # <title>UCI Machine Learning Repository: Data Sets</title>\n",
    "print(soup.title.get_text()) # UCI Machine Learning Repository: Data Sets\n",
    "print(soup.body) # 给出网站上的整个页面\n",
    "print(response.status_code)\n",
    "\n",
    "tables = soup.find_all('table', {'cellpadding':'3'})\n",
    "# 我们定位cellpadding属性值为3的表格\n",
    "# 我们可以使用id、class或HTML标签进行选择，有关更多信息，请查看beautifulsoup文档\n",
    "table = tables[0] # 结果是一个列表，我们从中提取数据\n",
    "for td in table.find('tr').find_all('td'):\n",
    "    print(td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e7120",
   "metadata": {},
   "source": [
    "如果你运行这段代码，你会发现提取工作只完成了一半。你可以继续完成它，因为这是练习1的一部分。\n",
    "参考[beautifulsoup文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)获取更多信息。\n",
    "\n",
    "🌕 你非常特别，你每天都在进步。你只剩下八天就要达到伟大的境界了。现在做一些练习来锻炼你的大脑和肌肉。\n",
    "\n",
    "## 💻 练习：第22天\n",
    "\n",
    "1. 抓取以下网站并将数据存储为json文件（url = 'http://www.bu.edu/president/boston-university-facts-stats/'）。\n",
    "2. 提取此url中的表格（https://archive.ics.uci.edu/ml/datasets.php）并将其更改为json文件。\n",
    "3. 抓取总统表并将数据存储为json（https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States）。这个表格结构不是很规整，抓取可能需要很长时间。\n",
    "\n",
    "🎉 恭喜！🎉\n",
    "\n",
    "[<< 第21天](./21_Day_Classes_and_objects/21_classes_and_objects_cn.md) | [第23天 >>](./23_Day_Virtual_environment/23_virtual_environment_cn.md)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
